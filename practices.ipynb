{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eb46f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\ASUS\\\\Desktop\\\\research work pdf\\\\ore mining\\\\GEOCHEM\\\\GEOCHEM\\\\wavelength.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(X_data), sample_names, np\u001b[38;5;241m.\u001b[39marray(wavelengths)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Example use\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m X, names, wavelengths \u001b[38;5;241m=\u001b[39m \u001b[43mload_geochem_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mROOT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWAVELENGTH_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWavelengths shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, wavelengths\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[1;32mIn[1], line 16\u001b[0m, in \u001b[0;36mload_geochem_dataset\u001b[1;34m(root_folder, wavelength_path)\u001b[0m\n\u001b[0;32m     13\u001b[0m sample_names \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Load the wavelength data\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwavelength_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     17\u001b[0m     wavelengths \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Traverse each GCH folder\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\ASUS\\\\Desktop\\\\research work pdf\\\\ore mining\\\\GEOCHEM\\\\GEOCHEM\\\\wavelength.json'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from sklearn.metrics import classification_report, confusion_matrix, cohen_kappa_score, roc_auc_score\n",
    "import seaborn as sns \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "batch_size = 32\n",
    "img_width = 224\n",
    "img_height = 224\n",
    "epochs = 50  # Number of epochs for training\n",
    "learning_rate = 0.0001  # Learning rate for Adam optimizer\n",
    "iterations = 100  # Number of iterations for the optimization process\n",
    "\n",
    "models = {\n",
    "         \"VGG16\": tf.keras.applications.VGG16,\n",
    "\"MobileNet\":tf.keras.applicatons.MobileNet\n",
    "#..\n",
    "#..\n",
    "#..\n",
    "#..\n",
    "#Deep Models\n",
    "#..\n",
    "\n",
    "}\n",
    "\n",
    "base_save_dir = Results\n",
    "if not os.path.exists(base_save_dir):\n",
    "    os.makedirs(base_save_dir)\n",
    "\n",
    "for fold_num in range(1, 6):\n",
    "    data_path = f'foldlar/fold_{fold_num}'\n",
    "    path_train = f\"{data_path}/train\"\n",
    "    path_test = f\"{data_path}/test\"\n",
    "    path_val = f\"{data_path}/test\"\n",
    "\n",
    "    # Augmenting the training data to synthetically increase dataset size\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255.,\n",
    "        rotation_range=10,  # Small rotations\n",
    "        width_shift_range=0.1,  # Horizontal shifting\n",
    "        height_shift_range=0.1,  # Vertical shifting\n",
    "        shear_range=0.2,  # Shearing\n",
    "        zoom_range=0.2,  # Zooming\n",
    "        brightness_range=[0.8, 1.2],  # Brightness adjustment\n",
    "        horizontal_flip=True,  # Horizontal flipping\n",
    "        fill_mode='nearest'  # Filling missing pixels\n",
    "    )\n",
    "    \n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        directory=path_train,\n",
    "        batch_size=64,\n",
    "        class_mode=\"categorical\",\n",
    "        target_size=(img_width, img_height)\n",
    "    )\n",
    "\n",
    "    # Validation and test sets will not be augmented\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255.)\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        directory=path_test,\n",
    "        batch_size=64,\n",
    "        class_mode=\"categorical\",\n",
    "        target_size=(img_width, img_height),\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    valid_datagen = ImageDataGenerator(rescale=1./255.)\n",
    "    valid_generator = valid_datagen.flow_from_directory(\n",
    "        directory=path_val,\n",
    "        batch_size=64,\n",
    "        class_mode=\"categorical\",\n",
    "        target_size=(img_width, img_height)\n",
    "    )\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name} on fold {fold_num}...\")\n",
    "        base_model = model(include_top=False, input_shape=(img_width, img_height, 3))\n",
    "        \n",
    "        x = GlobalAveragePooling2D(name=\"gap_layer\")(base_model.output)\n",
    "        outputs = Dense(3, activation='softmax')(x)\n",
    "        final_model = keras.Model(inputs=base_model.input, outputs=outputs)\n",
    "\n",
    "        # Using Adam optimizer with learning rate of 0.0001 as specified\n",
    "        final_model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                            loss='categorical_crossentropy',\n",
    "                            metrics=['accuracy'])\n",
    "\n",
    "        # Start training\n",
    "        start_train_time = time.time()\n",
    "        history = final_model.fit(train_generator,\n",
    "                                  epochs=epochs,\n",
    "                                  validation_data=valid_generator,\n",
    "                                  verbose=1)\n",
    "        train_duration = time.time() - start_train_time  # End timing\n",
    "\n",
    "        # Start testing\n",
    "        start_test_time = time.time()\n",
    "        y_pred_prob = final_model.predict(test_generator)\n",
    "        test_duration = time.time() - start_test_time  # End timing\n",
    "\n",
    "        y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "        y_true_ohe = tf.keras.utils.to_categorical(test_generator.classes, num_classes=3)\n",
    "\n",
    "        save_dir = os.path.join(base_save_dir, f\"{model_name}/fold_{fold_num}\")\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        model_save_path = os.path.join(save_dir, f\"{model_name}_fold_{fold_num}.hdf5\")\n",
    "        final_model.save(model_save_path)\n",
    "\n",
    "        metrics_save_path = os.path.join(save_dir, \"classification_metrics.txt\")\n",
    "        with open(metrics_save_path, 'w') as f:\n",
    "            kappa = cohen_kappa_score(test_generator.classes, y_pred)\n",
    "            auc_val = roc_auc_score(y_true_ohe, y_pred_prob, multi_class='ovr')\n",
    "            f.write(f\"Cohen's Kappa: {kappa}\\n\")\n",
    "            f.write(f\"AUC: {auc_val}\\n\")\n",
    "            f.write(f\"Training Time: {train_duration} seconds\\n\")\n",
    "            f.write(f\"Testing Time: {test_duration} seconds\\n\")\n",
    "\n",
    "            cm = confusion_matrix(test_generator.classes, y_pred)\n",
    "            f.write(\"\\nConfusion Matrix:\\n\")\n",
    "            f.write(str(cm))\n",
    "\n",
    "            report = classification_report(test_generator.classes, y_pred, digits=7, output_dict=True)\n",
    "            f1_score = report[\"weighted avg\"][\"f1-score\"]\n",
    "            precision = report[\"weighted avg\"][\"precision\"]\n",
    "            recall = report[\"weighted avg\"][\"recall\"]\n",
    "            accuracy = report[\"accuracy\"]\n",
    "\n",
    "            f.write(f\"\\n\\nPrecision (Weighted Avg): {precision}\\n\")\n",
    "            f.write(f\"Recall (Weighted Avg): {recall}\\n\")\n",
    "            f.write(f\"F1-Score (Weighted Avg): {f1_score}\\n\")\n",
    "            f.write(f\"Accuracy: {accuracy}\\n\")\n",
    "            f.write(\"\\n\\nFull Classification Report:\\n\")\n",
    "            f.write(classification_report(test_generator.classes, y_pred, digits=7))\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['accuracy'])\n",
    "        plt.plot(history.history['val_accuracy'])\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "        plt.savefig(os.path.join(save_dir, 'accuracy_plot.png'))\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('Model Loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "        plt.savefig(os.path.join(save_dir, 'loss_plot.png'))\n",
    "        plt.close()\n",
    "\n",
    "        df_cm = pd.DataFrame(cm, index=np.arange(cm.shape[0]), columns=np.arange(cm.shape[1]))\n",
    "        plt.figure(figsize=(10,7))\n",
    "        sns.set(font_scale=1.4)\n",
    "        sns.heatmap(df_cm, annot=True)\n",
    "        plt.savefig(os.path.join(save_dir, 'confusion_matrix.png'))\n",
    "        plt.close()\n",
    "\n",
    "        history_save_path = os.path.join(save_dir, 'history.txt')\n",
    "        with open(history_save_path, 'w') as f:\n",
    "            for key, value_list in history.history.items():\n",
    "                f.write(f\"{key}: {', '.join(map(str, value_list))}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4cd055",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
